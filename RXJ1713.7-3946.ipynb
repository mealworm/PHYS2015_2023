{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHYS2015A/PHYS3009A: MeV/GeV Astronomy Project\n",
    "\n",
    "Name: \n",
    "\n",
    "Student No.:\n",
    "\n",
    "total marks: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project you will do yourself the data analysis of the object RX J1713.7-3946. \n",
    "\n",
    "You will need to answer questions and write code.\n",
    "\n",
    "Please put your code in the cells starting with \n",
    "\n",
    "```\n",
    "# your code here\n",
    "```\n",
    "\n",
    "You will also need to answer some questions for marks. Edit the text within the cells, either providing descriptions of labeled parameters, or typing your answer where it says \"Answer here\".\n",
    "\n",
    "Do not change the code in any other cells. If you want to add additional ouput, or test some things you can create new cells for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood Analysis with fermipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python likelihood tools are a very powerful set of analysis tools that expand upon the command line tools provided with the Fermi Science Tools package. Not only can you perform all of the same likelihood analysis with the python tools that you can with the standard command line tools but you can directly access all of the model parameters. You can more easily script a standard analysis. There are also a few things built into the python tools that are not available from the command line like the calculation of upper limits.\n",
    "\n",
    "There are many user contributed packages built upon the python backbone of the Science Tools and this thread will highlight the use of the [fermipy](http://fermipy.readthedocs.org) package.\n",
    "\n",
    "This sample analysis of RX J1713.7-3946 is based on the PG 1553+113 analysis performed by the LAT team and described in [Abdo, A. A. et al. 2010, ApJ, 708, 1310](http://adsabs.harvard.edu/abs/2010ApJ...708.1310A) and closely follows the [Likelihood Analysis with Python](http://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/python_tutorial.html) thread.  This tutorial assumes you have the most recent ScienceTools installed and [fermipy](http://fermipy.readthedocs.org) installed on top of it.  For instructions on installing fermipy and the Fermi ScienceTools you should consult the [fermipy Installation Instructions](http://fermipy.readthedocs.org/en/latest/install.html).  We will also make significant use of python, so you might want to familiarize yourself with python including matplotlib and other libraries (there's a beginner's guide at http://wiki.python.org/moin/BeginnersGuide). This tutorial also assumes that you've gone through the non-python based [Unbinned Likelihood Tutorial](http://fermi.gsfc.nasa.gov/ssc/data/analysis/scitools/likelihood_tutorial.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this thread the original data were extracted from the [LAT data server](http://fermi.gsfc.nasa.gov/cgi-bin/ssc/LAT/LATDataQuery.cgi) with the following selections (these selections are similar to those in the 4FGL-DR4 paper):\n",
    "\n",
    "* Search Center (RA,Dec) = (258.112,-39.687)\n",
    "* Radius = 15 degrees\n",
    "* Start Time (MET) = 239557417 seconds (2008-08-04T15:43:36)\n",
    "* Stop Time (MET) = 681169985 seconds (2022-08-02T21:53:00)\n",
    "* Minimum Energy = 50 MeV\n",
    "* Maximum Energy = 1000000 MeV\n",
    "\n",
    "Let's retrieve the data. In this example the output of the analysis will go into a subdirectory called *data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import gdown\n",
    "\n",
    "if os.path.isdir('data'):\n",
    "    os.environ['DATA_DIR'] = 'data'\n",
    "else:\n",
    "    os.mkdir('data')\n",
    "    os.environ['DATA_DIR'] = 'data'\n",
    "\n",
    "drive_file_IDs = [\n",
    "[\"1AVK4mR-KfC0R4Be8bQ8paf1LPEi4fa9P\",\"data/L230831165314B24E85D632_PH00.fits\"],\n",
    "[\"1AylrtavbJMaGADh-bEJ6y9Z35d-q-hFk\",\"data/L230831165314B24E85D632_PH01.fits\"],\n",
    "[\"1B6Ogp9qA9Yb1TZ-AnqSIam3CEsccSFOC\",\"data/L230831165314B24E85D632_PH02.fits\"],\n",
    "[\"1B-1OGD9GtMK-_98AmNz-To7njJqzlUN8\",\"data/L230831165314B24E85D632_PH03.fits\"],\n",
    "[\"1B8LKOot0ZKER2J5FaEFAK6fGM1k2FZU8\",\"data/L230831165314B24E85D632_PH04.fits\"],\n",
    "[\"1B0iCHiwtRvjdd9W-9HWB0B0Y6Okz5SC3\",\"data/L230831165314B24E85D632_PH05.fits\"],\n",
    "[\"1m_XWgvAZdqxTjBY8Sa6uiLlJ3Fl4md4T\",\"data/L230831165314B24E85D632_SC00.fits.gz\"],\n",
    "[\"1mc5k5ZG_f0_QBoc9j7Xs6Z8pq1UqU_8b\",\"data/ltcube_00.fits\"]\n",
    "]\n",
    "for file in drive_file_IDs :\n",
    "    if not os.path.isfile(file[1]):\n",
    "        gdown.download(id=file[0],output=file[1],quiet=False)\n",
    "subprocess.run([\"gunzip\",\"data/L230831165314B24E85D632_SC00.fits.gz\"]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's retrieve the diffuse emission models. These come directly from the *fermipy-extras* package. We will put them in the *diffuse* subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.isdir('data'):\n",
    "    os.environ['DATADIR'] = 'data'\n",
    "\n",
    "if os.path.isfile('diffuse.tar.gz'):\n",
    "    !tar xzf diffuse.tar.gz\n",
    "else:\n",
    "    !curl -OL https://raw.githubusercontent.com/fermiPy/fermipy-extras/master/data/diffuse.tar.gz\n",
    "    !tar xzf diffuse.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a file list\n",
    "\n",
    "You'll then need to make a file list with the names of your input event files. You can either just make one with a text editor or do the following from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -1 data/*PH*.fits > data/RXJ1713.7-3946.lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a config file\n",
    "\n",
    "fermipy bases its analysis on a configuration file (in [yaml](http://yaml.org) format).  We're just going to use a really simple config file for a standard analysis.  There are many many more options which you can use or you can modify these options after the fact within the analysis chain.\n",
    "\n",
    "\n",
    "Make a config file named 'config.yaml' like the following.  For more details on the config file see [config.html](http://fermipy.readthedocs.org/en/latest/config.html).  You will probably need to customize this a bit since your files might not be in the same place or named the same.  The galactic and isotropic diffuse will need to be located on your system (they are included in the science tools or can be downloaded from the FSSC).  In the following example we set the path to these files with the environment variable FERMI_DIFFUSE_DIR.  If FERMI_DIFFUSE_DIR is not defined fermipy will look for the location of these files within the FSSC STs distribution. \n",
    "\n",
    "```\n",
    "data:\n",
    "  evfile : RXJ1713.7-3946.lst\n",
    "  scfile : data/L211025154921445A2D9929_SC00.fits\n",
    "\n",
    "binning:\n",
    "  roiwidth   : 10.0\n",
    "  binsz      : 0.1\n",
    "  binsperdec : 8\n",
    "\n",
    "selection :\n",
    "  emin : 100\n",
    "  emax : 300000\n",
    "  tmin : 239557417\n",
    "  tmax : 681169985\n",
    "  zmax    : 105\n",
    "  evclass : 128\n",
    "  evtype  : 3\n",
    "  target : 'RXJ1713.7-3946'\n",
    "\n",
    "gtlike:\n",
    "  edisp : True\n",
    "  irfs : 'P8R2_SOURCE_V6'\n",
    "  edisp_disable : ['isodiff','galdiff']\n",
    "\n",
    "model:\n",
    "  src_roiwidth : 15.0\n",
    "  galdiff  : 'diffuse/gll_iem_v07.fits'\n",
    "  isodiff  : 'diffuse/iso_P8R2_SOURCE_V6_v06.txt'\n",
    "  catalogs : ['4FGL']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the analysis\n",
    "\n",
    "Next, you create an analysis script and run the setup steps which include running the selections and generating exposure maps etc.  This will take a bit.\n",
    "\n",
    "This is where the magic happens.  fermipy will load the point source model, create your xml file for you, decide on all the appropriate cuts and binnings and just go.  All of this is configurable from python or from the config file.  And, if you need to rerun things, it's smart enough to not overwrite files if it doesn't need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load up some useful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the GTAnalysis module from fermipy\n",
    "\n",
    "You start by importing the module and then creating an instance of the analysis object from our config file.  When instantiating the analysis object we can override any options defined in the configuration file by passing keyword arguments to the object constructor.  Here we explicitly set the verbosity parameter to 3 (INFO) which supresses DEBUG output.  When we create the object, it spits out a bunch of information about all of the parameters that were used.  You can see there are many more options than the ones we chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fermipy.gtanalysis import GTAnalysis\n",
    "gta = GTAnalysis('data/config.yaml',logging={'verbosity': 3})\n",
    "matplotlib.interactive(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The setup routine\n",
    "\n",
    "This gets everything ready for the likelihood analysis including instantiating the pylikelihood object.  Note that fermipy will skip generating any ancillary files that already exist in the working directory.  In the sample tarball these files have already been produced in order to speed up this stage of the analysis.  If you want to see the behavior of fermipy when running from an empty working directory you can delete one or more of these files before running *setup*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[1 mark]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with the analysis we'll have a quick look at the files that are produced by the setup function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls data/*.fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following files, write a brief explanation of the contents of each file and its role in the analysis: **[7 marks]**\n",
    "\n",
    "* **ft1_00.fits**: \n",
    "* **bexpmap_00.fits**: \n",
    "* **bexpmap_roi_00.fits**: \n",
    "* **ccube_00.fits**: \n",
    "* **ltcube_00.fits**: \n",
    "* **srcmap_00.fits**: \n",
    "\n",
    "To see example of one of these files we can open and plot the counts cube file.  This is a 3D cube that contains the distribution of events as a function of energy and two spatial coordinates.  In the example below we sum over the energy dimension of the cube to make a 2-D sky image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.io.fits as pyfits\n",
    "\n",
    "h = pyfits.open('data/ccube.fits')\n",
    "h.info()\n",
    "counts = h[0].data\n",
    "counts.shape\n",
    "plt.figure()\n",
    "plt.imshow(np.sum(counts,axis=0),interpolation='nearest',origin='lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the state of the ROI prior with the print_roi() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[1 mark]**\n",
    "\n",
    "How many catalog point sources are there within the ROI? **[1 mark]**\n",
    "\n",
    "Answer here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional details about an individual source can be retrieved by printing the corresponding source object.  Here we use the bracket operator to return the properties of RXJ1713.7-3946. The catalog name of PKS 2155-403 is '4FGL J1713.5-3945e'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name = '4FGL J1713.5-3945e'\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[1 mark]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the SpatialModel is SpatialMap rather than PointSource. An intensity map taken from another instrument (in this case HESS) is used as input to the model rather than assuming that all of the photons come from only one direction. The shape and relative intensities are fixed; only the normalization is left as a free parameter. Let's take a look at the template used for this source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = pyfits.open('data/RXJ1713_2016_250GeV.fits')\n",
    "h2.info()\n",
    "template = h2[0].data\n",
    "template.shape\n",
    "plt.figure()\n",
    "plt.imshow(template,interpolation='nearest',origin='lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the spatial map change the number of free parameters in the model compared to a point source? Why or why not? **[1 mark]**\n",
    "\n",
    "Answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the likelihood fitting\n",
    "\n",
    "Now that all of the ancillary files have been generated, we can move on to the actual fitting.  The first thing you should do is free some of the sources since all of the sources are initially fixed.  We'll just free those sources in the center region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Normalization of all Sources within 3 deg of ROI center\n",
    "# your code here\n",
    "\n",
    "# Free all parameters of isotropic and galactic diffuse components\n",
    "# your code here (two lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[3 marks]**\n",
    "\n",
    "What is the purpose of freeing the normalization of sources within 3 degrees? How are the normalizations affected when fitting the null hypothesis? **[2 mark]**\n",
    "\n",
    "Answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple anlaysis we are leaving the spectral shapes of sources fixed but we're going to free the spectral shape of the source we care about.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[1 mark]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, actually do the fit.  The software does its best to get the fit to converge by running the fit several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[1 mark]**\n",
    "\n",
    "Let's look at the fit results and the quality of the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fit Quality: ',fit_results['fit_quality'],', Fit status: ',fit_results['fit_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the significance of these two numbers? What do we expect them to be? **[4 marks]**\n",
    "\n",
    "Answer here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can improve the quality of the fit by running it again, with starting values obtained from the previous fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[1 mark]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary returned by the fit method returns a variety of diagnostic information about the fit including the fit quality, the relative improvement in the likelihood, and the correlations among the fit parameters.  We can inspect the results of the fit by printing the source object for RXJ1713.7-3946."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fit Quality: ',fit_results['fit_quality'],', Fit status: ',fit_results['fit_status'])\n",
    "print(gta.roi[source_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the meaning of the following parameters: **[7 marks]**\n",
    "\n",
    "\n",
    "* **TS**             : \n",
    "* **Npred**          : \n",
    "* **Flux**           : \n",
    "* **EnergyFlux**     : \n",
    "* **b'norm'**        :  \n",
    "* **b'alpha'**       :       \n",
    "* **b'beta'**        :    \n",
    "* **b'Eb'**          :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then save the state of the roi to an output file for reference later.  The write_roi function does this.  The first argument is a string that will be prepended to the names of the output files generated by this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[1 mark]**\n",
    "\n",
    "There are a lot of diagnostic plots also saved at the same time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l data/*.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pngs = glob('data/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for png in pngs:\n",
    "    my_image = Image(png)\n",
    "    display(my_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a short description of each of the five figures: **[5 marks]**\n",
    "\n",
    "Answer here:\n",
    "\n",
    "What is the dominant component of the spectrum **[1 mark]**\n",
    "\n",
    "Answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the results\n",
    "\n",
    "Since the results are saved, you can load them back up at any point (you can also get to these within python).  Here we retrieve the analysis results from the output numpy file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.load('data/fit0.npy', allow_pickle=True).flat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sources` dictionary has an entry for each source in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted(c['sources'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the flux, spectral parameters, and TS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c['sources'][source_name]['flux']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c['sources'][source_name]['param_names'][:4])\n",
    "print(c['sources'][source_name]['param_values'][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c['sources'][source_name]['ts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SED is in there as well.  We can plot it.\n",
    "\n",
    "What does SED stand for? **[1 mark]**\n",
    "\n",
    "Answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.array(c['sources'][source_name]['model_flux']['energies'])\n",
    "dnde = np.array(c['sources'][source_name]['model_flux']['dnde'])\n",
    "dnde_hi = np.array(c['sources'][source_name]['model_flux']['dnde_hi'])\n",
    "dnde_lo = np.array(c['sources'][source_name]['model_flux']['dnde_lo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(E, (E**2)*dnde, 'k--')\n",
    "plt.loglog(E, (E**2)*dnde_hi, 'k')\n",
    "plt.loglog(E, (E**2)*dnde_lo, 'k')\n",
    "plt.xlabel('E [MeV]')\n",
    "plt.ylabel(r'E$^2$ dN/dE [MeV cm$^{-2}$ s$^{-1}$]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want SED points, there's a function for that.  There are lots of options for this which you can set in the config file or from keyword arguments of the function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output produced: **[1 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save the state to the yaml file or you can just access it directly.  This is also the way to get at the dictionary for any individual source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = gta.roi[source_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.loglog(E, (E**2)*dnde, 'k--')\n",
    "plt.loglog(E, (E**2)*dnde_hi, 'k')\n",
    "plt.loglog(E, (E**2)*dnde_lo, 'k')\n",
    "plt.errorbar(np.array(sed['e_ctr']),\n",
    "             sed['e2dnde'], \n",
    "             yerr=sed['e2dnde_err'], fmt ='o')\n",
    "plt.xlabel('E [MeV]')\n",
    "plt.ylabel(r'E$^{2}$ dN/dE [MeV cm$^{-2}$ s$^{-1}$]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table\n",
    "sed_tab = Table.read('data/4fgl_j1713.5-3945e_sed.fits')\n",
    "sed_tab.write('data/4fgl_j1713.5-3945e_sed.ecsv',exclude_names=['norm_scan','dloglike_scan'],overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This is it. You have done an analysis of the data recorded with Fermi-LAT on RXJ1713.7-3946."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check\n",
    "\n",
    "Before you submit your work you should make a few checks that everything works fine.\n",
    "\n",
    "1. Save your notebook as a PDF (File->Print...->Print as PDF). This document will help you debugging in the next step.\n",
    "1. Restart the kernel and rerun the entire notebook (Kernel->Restart Kernel & Run All Cells...). This will delete all variables (but not your code) and rerun the notebook in one go. If this does not go through the end (i.e. you do not see the output of the next cell) then you have to fix it. You will see at which cell the run stopped. A common mistake is using a variable that is defined only at a later stage.\n",
    "1. You think you fixed everything? Redo step 2 (Kernel->Restart Kernel & Run All Cells...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a\\bYa\\boa\\bua\\b a\\baa\\bra\\bea\\b a\\bra\\bea\\baa\\bda\\bya\\b a\\bta\\boa\\b a\\bsa\\bua\\bba\\bma\\bia\\bta\\b.a\\b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see the output of the last cell without explicitly running it? Then the notebook goes through with one kernel restart. You can proceed to submission.\n",
    "You do not see the output? Go back to step 2 above.\n",
    "\n",
    "The jupyter notebook goes through all cells with one Kernel->Restart Kernel & Run All Cells...    **[1 mark]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "You have to download and submit 3 files.\n",
    "- PDF file. File->Print...->Print as PDF. Save this file on your disk. If this doesn't work you can also try File->Export Notebook As->HTML. This file serves as reference for what you have done.\n",
    "- Jupyter notebook. File->Download. Save this file on your disk. This file will be tested and you receive your marks for that.\n",
    "- Spectral data points. Go to the Home Page of your jupyter notebooks. It is in a different tab or window of your browser. Find the file `data/4fgl_j1713.5-3945e_sed.ecsv` and save it to your disk. You will need this file at a later stage.\n",
    "\n",
    "Please submit all 3 files on Ulwazi.        **[1 mark]**\n",
    "\n",
    "Congratulations. You have succesfully completed your MeV/GeV Astronomy project."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "22bb53b43b3afb0848b0afa27adb8f2415fb61399be88b9ea69bd0b84fe58c05"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
